# app.py
# ---------------------------
# Knowledge Transfer App (Google Sheets + –ª–æ–∫–∞–ª—å–Ω—ã–π fallback)
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:
# - –∑–∞–ø–∏—Å—å –≤ Google Sheets: update —Å A1-–¥–∏–∞–ø–∞–∑–æ–Ω–æ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–æ–ª–æ–Ω–∫–∏ > 'Z'
# - —á—Ç–µ–Ω–∏–µ credentials –∏–∑ st.secrets (–∏–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞)
# - –≥–∞—Ä–∞–Ω—Ç–∏—è –∞—Ä–∫—É—à–µ–π Lakes/Reports
# ---------------------------

import streamlit as st
from datetime import datetime
import os
import pandas as pd
import plotly.express as px
from PIL import Image
import base64
import requests
import time
import json

try:
    from openpyxl import load_workbook  # noqa: F401
except ImportError:
    pass

# ===== Google Sheets (–Ω–æ–≤—ã–µ) =====
try:
    import gspread
    from google.oauth2.service_account import Credentials
    from gspread.utils import rowcol_to_a1
    GOOGLE_SHEETS_AVAILABLE = True
    GS_IMPORT_ERROR = ""
except Exception as e:
    GOOGLE_SHEETS_AVAILABLE = False
    GS_IMPORT_ERROR = str(e)

# ==== CONFIG SECTION ====
# –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π
LOCAL_DATA_DIR = os.path.join(os.path.expanduser("~"), "AppData", "Local", "StreamlitData")
os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
EXCEL_FILE_PATH = os.path.join(LOCAL_DATA_DIR, "LakeHouse.xlsx")

# Google Sheets ID (–∑–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
GOOGLE_SHEETS_ID = "19Ge1PiHdeWt0mofW5YkxmectUchGcbclaHNim_XvmFM"
# –ß—Ç–µ–Ω–∏–µ –∏–∑ Google Sheets (CSV —á–µ—Ä–µ–∑ gviz) ‚Äî –ª–∏—Å—Ç—ã Lakes/Reports
GOOGLE_SHEETS_URL_LAKES = f"https://docs.google.com/spreadsheets/d/{GOOGLE_SHEETS_ID}/gviz/tq?tqx=out:csv&sheet=Lakes"
GOOGLE_SHEETS_URL_REPORTS = f"https://docs.google.com/spreadsheets/d/{GOOGLE_SHEETS_ID}/gviz/tq?tqx=out:csv&sheet=Reports"

# ----------------- –£—Ç–∏–ª–∏—Ç—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è -----------------
def display_image_from_path(image_path, caption=None, width=None):
    try:
        if image_path.startswith(('http://', 'https://')):
            st.image(image_path, caption=caption, width=width)
        elif os.path.exists(image_path):
            image = Image.open(image_path)
            st.image(image, caption=caption, width=width)
        else:
            st.warning(f"‚ö†Ô∏è –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {image_path}")
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")

def display_image_from_base64(base64_string, caption=None, width=None):
    try:
        image_data = base64.b64decode(base64_string)
        st.image(image_data, caption=caption, width=width)
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}")

def process_text_with_images(text: str):
    if not text:
        return text
    import re
    image_pattern = r'\[IMAGE:(.*?)\]'
    matches = re.findall(image_pattern, text)
    if matches:
        parts = re.split(image_pattern, text)
        for i, part in enumerate(parts):
            if i % 2 == 0:
                if part.strip():
                    st.markdown(part)
            else:
                image_path = part.strip()
                if image_path.startswith('C:\\') and 'PL-notebook.png' in image_path:
                    github_url = "https://raw.githubusercontent.com/AleksandraFilatova/knowledge-transfer-app/main/Image/Sac-notebook.PNG"
                    display_image_from_path(github_url, width=600)
                elif 'github.com' in image_path and '/blob/' in image_path:
                    raw_url = image_path.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                    display_image_from_path(raw_url, width=600)
                else:
                    display_image_from_path(image_path, width=600)
    else:
        st.markdown(text)

# ----------------- –ß—Ç–µ–Ω–∏–µ Excel –ª–æ–∫–∞–ª—å–Ω–æ -----------------
@st.cache_data(ttl=300)
def load_lakes_and_reports(excel_path):
    try:
        xl = pd.ExcelFile(excel_path, engine='openpyxl')
        available_sheets = xl.sheet_names

        lakes_df = pd.read_excel(xl, 'Lakes', engine='openpyxl') if 'Lakes' in available_sheets else \
                   pd.read_excel(xl, available_sheets[0], engine='openpyxl')

        reports_df = pd.read_excel(xl, 'Reports', engine='openpyxl') if 'Reports' in available_sheets else \
                     pd.DataFrame()

        # –Ω–∞–∑–≤–∞–Ω–∏—è (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
        lakes_names = list(lakes_df['LakeHouse'].dropna().unique()) if 'LakeHouse' in lakes_df.columns else list(lakes_df.iloc[:,0].dropna().unique())
        reports_names = list(reports_df.iloc[:,0].dropna().unique()) if not reports_df.empty else []
        return lakes_names, reports_names, lakes_df, reports_df

    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ —Ñ–∞–π–ª—É: {e}")
        st.warning("üí° –ó–∞–∫—Ä–∏–π—Ç–µ —Ñ–∞–π–ª –≤ Excel, –¥–æ—á–µ–∫–∞–π—Ç–µ—Å—å —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó OneDrive, –æ–Ω–æ–≤—ñ—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫—É.")
        return [], [], None, None

def create_default_excel_file(local_path):
    try:
        default_data = {
            'LakeHouse': [], 'Folder': [], 'Element': [], 'URL': [],
            '–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫': [], '–í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω': []
        }
        df = pd.DataFrame(default_data)
        with pd.ExcelWriter(local_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Lakes', index=False)
            df.to_excel(writer, sheet_name='Reports', index=False)
        return True
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—É: {e}")
        return False

def save_data_to_excel(df, filename, reports_table=None):
    try:
        st.info(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–µ –ª–æ–∫–∞–ª—å–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {filename}")
        with pd.ExcelWriter(filename, engine='openpyxl', mode='w') as writer:
            df.to_excel(writer, sheet_name='Lakes', index=False)
            if reports_table is not None and not reports_table.empty:
                reports_table.to_excel(writer, sheet_name='Reports', index=False)
        st.success(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {os.path.abspath(filename)}")
        return True, filename
    except PermissionError as e:
        st.error(f"‚ùå –î–æ—Å—Ç—É–ø –¥–æ —Ñ–∞–π–ª—É: {e}")
        return False, None
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ: {type(e).__name__}: {e}")
        return False, None

# ----------------- –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ (–≤–∏–∑—É–∞–ª–∫–∏) -----------------
def analyze_lakes_data(lakes_df: pd.DataFrame):
    if lakes_df is None or lakes_df.empty:
        return {'total_lakes': 0, 'columns': [], 'missing_data': {}, 'unique_values': {}}
    analysis = {
        'total_lakes': len(lakes_df),
        'columns': list(lakes_df.columns),
        'missing_data': {c: lakes_df[c].isna().sum() for c in lakes_df.columns},
        'unique_values': {}
    }
    for c in lakes_df.columns:
        if lakes_df[c].dtype == 'object':
            analysis['unique_values'][c] = lakes_df[c].value_counts().to_dict()
    return analysis

def create_lakes_visualization(lakes_df):
    if lakes_df is None or lakes_df.empty:
        return None
    charts = {}
    if 'Status' in lakes_df.columns:
        status_counts = lakes_df['Status'].value_counts()
        charts['status_pie'] = px.pie(values=status_counts.values, names=status_counts.index,
                                      title="–†–æ–∑–ø–æ–¥—ñ–ª –ª–µ–π–∫—ñ–≤ –∑–∞ —Å—Ç–∞—Ç—É—Å–æ–º")
    if 'Update_Frequency' in lakes_df.columns:
        freq = lakes_df['Update_Frequency'].value_counts()
        charts['frequency_bar'] = px.bar(x=freq.index, y=freq.values, title="–ß–∞—Å—Ç–æ—Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω—å –ª–µ–π–∫—ñ–≤")
    if 'Workspace' in lakes_df.columns:
        charts['workspace_treemap'] = px.treemap(lakes_df, path=['Workspace'], title="–†–æ–∑–ø–æ–¥—ñ–ª –ª–µ–π–∫—ñ–≤ –ø–æ —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ—Å—Ç–æ—Ä–∞—Ö")
    return charts

def create_lake_details_card(lake_row: pd.Series):
    if lake_row is None or lake_row.empty:
        return "–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –ø—Ä–æ –ª–µ–π–∫"
    card_html = f"""
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px; border-radius: 10px; color: white; margin: 10px 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    ">
        <h3 style="margin: 0 0 15px 0; color: white;">üèûÔ∏è {lake_row.get('LakeHouse', '–ù–µ–≤—ñ–¥–æ–º–∏–π –ª–µ–π–∫')}</h3>
    """
    for col in lake_row.index:
        if pd.notna(lake_row[col]) and col not in ['LakeHouse', 'Folder', 'Element', '–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫', '–í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω']:
            display_col_name = {'Type': '–¢–∏–ø', '–û–ø–∏—Å': '–û–ø–∏—Å', '–û–Ω–æ–≤–ª–µ–Ω–Ω—è': '–û–Ω–æ–≤–ª–µ–Ω–Ω—è', '–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ': '–û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ'}.get(col, col)
            card_html += f'<p style="margin:5px 0;"><strong>{display_col_name}:</strong> {lake_row[col]}</p>'
    card_html += "</div>"
    return card_html

# ----------------- –ß—Ç–µ–Ω–∏–µ –∏–∑ Google Sheets (CSV) -----------------
def load_from_google_sheets():
    try:
        lakes_df = pd.read_csv(GOOGLE_SHEETS_URL_LAKES)
        try:
            reports_df = pd.read_csv(GOOGLE_SHEETS_URL_REPORTS)
        except Exception:
            reports_df = pd.DataFrame()
        lakes_names = list(lakes_df['LakeHouse'].dropna()) if 'LakeHouse' in lakes_df.columns else list(lakes_df.iloc[:,0].dropna())
        reports_names = list(reports_df.iloc[:,0].dropna()) if not reports_df.empty else []
        return lakes_names, reports_names, lakes_df, reports_df
    except Exception as e:
        st.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ Google Sheets (—á–∏—Ç–∞–Ω–Ω—è): {e}")
        return [], [], None, None

# ----------------- –ó–ê–ü–ò–° –≤ Google Sheets (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π) -----------------
def _get_gspread_client():
    """
    1) –ø—Ä–æ–±—É–µ–º st.secrets['gcp_service_account'] (dict –∏–ª–∏ JSON-—Å—Ç—Ä–æ–∫–∞)
    2) –∏–Ω–∞—á–µ —Ñ–∞–π–ª service_account_credentials.json (—Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º –∏–ª–∏ –≤ –¥–æ–º–∞—à–Ω–µ–π –ø–∞–ø–∫–µ)
    """
    if not GOOGLE_SHEETS_AVAILABLE:
        raise RuntimeError(f"gspread/google-auth –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {GS_IMPORT_ERROR}")

    scopes = ["https://www.googleapis.com/auth/spreadsheets",
              "https://www.googleapis.com/auth/drive"]

    # —á–µ—Ä–µ–∑ st.secrets (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è Streamlit Cloud)
    if "gcp_service_account" in st.secrets:
        sa_info = st.secrets["gcp_service_account"]
        if isinstance(sa_info, str):
            sa_info = json.loads(sa_info)
        creds = Credentials.from_service_account_info(sa_info, scopes=scopes)
        return gspread.authorize(creds)

    # —Ñ–∞–π–ª JSON
    here = os.path.dirname(os.path.abspath(__file__))
    candidates = [
        os.path.join(here, "service_account_credentials.json"),
        os.path.join(os.path.expanduser("~"), "service_account_credentials.json")
    ]
    for path in candidates:
        if os.path.exists(path):
            creds = Credentials.from_service_account_file(path, scopes=scopes)
            return gspread.authorize(creds)

    raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á —Å–µ—Ä–≤–∏—Å-–∞–∫–∫–∞—É–Ω—Ç–∞: –ø–æ–ª–æ–∂–∏ JSON –≤ st.secrets['gcp_service_account'] "
                            "–∏–ª–∏ —Ñ–∞–π–ª service_account_credentials.json —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º/–≤ –¥–æ–º–∞—à–Ω–µ–π –ø–∞–ø–∫–µ.")

def _ensure_worksheet(sh, title, rows=1000, cols=50):
    try:
        return sh.worksheet(title)
    except gspread.WorksheetNotFound:
        return sh.add_worksheet(title=title, rows=rows, cols=cols)

def _update_sheet_with_dataframe(ws, df: pd.DataFrame):
    if df is None or df.empty:
        ws.clear()
        return
    # –∑–Ω–∞—á–µ–Ω–∏—è: –∑–∞–≥–æ–ª–æ–≤–∫–∏ + —Å—Ç—Ä–æ–∫–∏; –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ NaN –∫ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º
    values = [df.columns.tolist()] + df.fillna("").astype(str).values.tolist()
    last_row = len(values)
    last_col = len(values[0]) if values else 1
    end_a1 = rowcol_to_a1(last_row, last_col)   # –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏ –ø–æ—Å–ª–µ 'Z'
    ws.clear()
    ws.update(f"A1:{end_a1}", values, value_input_option="RAW")

def save_to_google_sheets(df: pd.DataFrame, reports_table: pd.DataFrame | None = None) -> bool:
    try:
        gc = _get_gspread_client()
        sh = gc.open_by_key(GOOGLE_SHEETS_ID)

        # –í–ê–ñ–ù–û: –ø–æ–¥–µ–ª–∏—Å—å —Ç–∞–±–ª–∏—Ü–µ–π —Å client_email —Å–µ—Ä–≤–∏—Å-–∞–∫–∫–∞—É–Ω—Ç–∞ (Editor)!
        lakes_ws = _ensure_worksheet(sh, "Lakes", rows=max(1000, len(df)+10), cols=max(20, len(df.columns)+2))
        _update_sheet_with_dataframe(lakes_ws, df)

        if reports_table is not None and not reports_table.empty:
            reports_ws = _ensure_worksheet(sh, "Reports",
                                           rows=max(1000, len(reports_table)+10),
                                           cols=max(20, len(reports_table.columns)+2))
            _update_sheet_with_dataframe(reports_ws, reports_table)

        st.success("‚úÖ –î–∞–Ω—ñ —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ Google Sheets!")
        return True

    except gspread.exceptions.APIError as api_err:
        st.error(f"‚ùå Google API error: {api_err}")
        st.info("üîé –ü–µ—Ä–µ–≤—ñ—Ä: 1) —Å–µ—Ä–≤—ñ—Å-–∞–∫–∞—É–Ω—Ç –º–∞—î –¥–æ—Å—Ç—É–ø (Editor) –¥–æ —Ç–∞–±–ª–∏—Ü—ñ; 2) ID —Ç–∞–±–ª–∏—Ü—ñ –≤—ñ—Ä–Ω–∏–π; 3) –Ω–∞–∑–≤–∏ –ª–∏—Å—Ç—ñ–≤ 'Lakes'/'Reports'.")
        return False
    except FileNotFoundError as cred_err:
        st.error(f"‚ùå –ö—Ä–µ–¥–µ–Ω—à—ñ–∞–ª–∏: {cred_err}")
        return False
    except Exception as e:
        st.error(f"‚ùå –ù–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ Google Sheets: {e}")
        return False

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–û–†–Ü–ù–ö–ò ====================
st.set_page_config(page_title="Knowledge Transfer App", page_icon="üß†", layout="wide", initial_sidebar_state="expanded")

# ==================== –ù–ê–í–Ü–ì–ê–¶–Ü–Ø ====================
st.sidebar.title("üóÇÔ∏è –ù–∞–≤—ñ–≥–∞—Ü—ñ—è")
st.sidebar.markdown("### –û–±–µ—Ä—ñ—Ç—å —Ä–æ–∑–¥—ñ–ª:")
section = st.sidebar.radio("", ["üè† –ì–æ–ª–æ–≤–Ω–∞", "üíß –û–Ω–æ–≤–ª–µ–Ω–Ω—è LakeHouses", "üìä –û–Ω–æ–≤–ª–µ–Ω–Ω—è PowerBI Report", "‚úèÔ∏è –†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö", "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–∞ —Ä–µ—Å—É—Ä—Å–∏"])
st.sidebar.markdown("---")
st.sidebar.info(f"üìÖ –û—Å—Ç–∞–Ω–Ω—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:\n{datetime.now().strftime('%d.%m.%Y')}")

# –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø–æ –∫—Ä–µ–¥–∞–º (–µ—Å–ª–∏ –Ω–µ—Ç st.secrets –∏ —Ñ–∞–π–ª–∞)
if not ("gcp_service_account" in st.secrets):
    CREDENTIALS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "service_account_credentials.json")
    if not os.path.exists(CREDENTIALS_FILE):
        st.sidebar.markdown("---")
        st.sidebar.warning("‚ö†Ô∏è Google Sheets credentials –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        uploaded_credentials = st.sidebar.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ service_account_credentials.json", type=['json'], key='credentials_upload')
        if uploaded_credentials is not None:
            with open(CREDENTIALS_FILE, "wb") as f:
                f.write(uploaded_credentials.getbuffer())
            st.sidebar.success("‚úÖ Credentials –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")
            st.rerun()

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —Å–ø–µ—Ä–≤–∞ Google Sheets (CSV), –∑–∞—Ç–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π fallback ===
lakes, reports, lakes_table, reports_table = load_from_google_sheets()

if lakes_table is not None and not lakes_table.empty:
    st.sidebar.success(f"‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ Google Sheets ({len(lakes_table)} —Ä—è–¥–∫—ñ–≤)")
else:
    if os.path.exists(EXCEL_FILE_PATH):
        lakes, reports, lakes_table, reports_table = load_lakes_and_reports(EXCEL_FILE_PATH)
        st.sidebar.info(f"üìÇ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –ª–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª: `{os.path.abspath(EXCEL_FILE_PATH)}`")
    else:
        st.warning("‚ö†Ô∏è –§–∞–π–ª LakeHouse.xlsx –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel —Ñ–∞–π–ª:")
        uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel —Ñ–∞–π–ª", type=['xlsx', 'xls'])
        if uploaded_file is not None:
            with open(EXCEL_FILE_PATH, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.success("‚úÖ –§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ! –û–Ω–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ...")
            st.cache_data.clear()
            lakes, reports, lakes_table, reports_table = load_lakes_and_reports(EXCEL_FILE_PATH)
            st.sidebar.info(f"üìÇ –õ–æ–∫–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª: `{os.path.abspath(EXCEL_FILE_PATH)}`")
        else:
            lakes, reports, lakes_table, reports_table = [], [], None, None
            st.info("üëÜ –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel —Ñ–∞–π–ª –∞–±–æ –ø—ñ–¥–∫–ª—é—á—ñ—Ç—å Google Sheets —É —Å–∞–π–¥–±–∞—Ä—ñ")

# ==================== –ì–û–õ–û–í–ù–ê –°–¢–û–†–Ü–ù–ö–ê ====================
if section == "üè† –ì–æ–ª–æ–≤–Ω–∞":
    st.header("–í—ñ—Ç–∞—î–º–æ! üëã")
    st.markdown(f"""
    –¶—è –±–∞–∑–∞ –∑–Ω–∞–Ω—å –º—ñ—Å—Ç–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞—à–∏—Ö LakeHouses —Ç–∞ Power BI Reports.
    """)
    col1, col2 = st.columns(2)
    with col1: st.metric("üèûÔ∏è Data Lakes", len(lakes) if lakes else 0)
    with col2: st.metric("üìä Power BI –∑–≤—ñ—Ç–∏", len(reports) if reports else 0)

# ==================== –û–ù–û–í–õ–ï–ù–ù–Ø DATA LAKES ====================
elif section == "üíß –û–Ω–æ–≤–ª–µ–Ω–Ω—è LakeHouses":
    st.header("üíß –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –ø–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—é LakeHouses")
    unique_lakes = []
    if lakes_table is not None and not lakes_table.empty:
        for col in ['LakeHouse', 'name', 'Name', '–Ω–∞–∑–≤–∞', '–ù–∞–∑–≤–∞', 'lake_name', 'Lake Name', 'Lakehouse']:
            if col in lakes_table.columns:
                unique_lakes = list(lakes_table[col].dropna().unique())
                break
        if not unique_lakes:
            unique_lakes = list(lakes_table.iloc[:,0].dropna().unique())

    lake_select_options = ["–í—Å—ñ –ª–µ–π–∫–∏"] + unique_lakes + ["üìä –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è"]
    lake_name = st.selectbox("–û–±–µ—Ä—ñ—Ç—å Data Lake:", lake_select_options)

    if lake_name == "–í—Å—ñ –ª–µ–π–∫–∏":
        st.info("üëà –û–±–µ—Ä—ñ—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –ª–µ–π–∫ –∑—ñ —Å–ø–∏—Å–∫—É –≤–∏—â–µ")
        if lakes_table is not None and not lakes_table.empty and 'LakeHouse' in lakes_table.columns:
            unique_lakes_vals = lakes_table['LakeHouse'].dropna().unique()
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("üèûÔ∏è –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ª–µ–π–∫—ñ–≤", len(unique_lakes_vals))
            st.subheader("üìã –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö Data Lakes")
            if '–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫' in lakes_table.columns:
                summary = lakes_table.groupby('LakeHouse').first().reset_index()
                st.dataframe(summary[['LakeHouse','–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫']], use_container_width=True, hide_index=True)
            else:
                st.dataframe(lakes_table[['LakeHouse']], use_container_width=True, hide_index=True)
        else:
            st.warning("–°–ø–∏—Å–æ–∫ –ª–µ–π–∫—ñ–≤ –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –≤—ñ–¥—Å—É—Ç–Ω—è –∫–æ–ª–æ–Ω–∫–∞ 'LakeHouse'.")
    elif lake_name == "üìä –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è":
        st.subheader("üìä –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ª–µ–π–∫—ñ–≤")
        if lakes_table is not None and not lakes_table.empty:
            analysis = analyze_lakes_data(lakes_table)
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("üèûÔ∏è –í—Å—å–æ–≥–æ –ª–µ–π–∫—ñ–≤", analysis['total_lakes'])
            c2.metric("üìä –ö–æ–ª–æ–Ω–æ–∫ –¥–∞–Ω–∏—Ö", len(analysis['columns']))
            c3.metric("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å", sum(analysis['missing_data'].values()))
            c4.metric("üìÖ –û—Å—Ç–∞–Ω–Ω—î –æ–Ω–æ–≤–ª–µ–Ω–Ω—è", datetime.now().strftime('%d.%m'))
            charts = create_lakes_visualization(lakes_table)
            if charts:
                for chart in charts.values():
                    st.plotly_chart(chart, use_container_width=True)
            st.subheader("üîç –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑")
            missing_df = pd.DataFrame(list(analysis['missing_data'].items()), columns=['–ö–æ–ª–æ–Ω–∫–∞','–ü—Ä–æ–ø—É—â–µ–Ω–æ'])
            missing_df = missing_df[missing_df['–ü—Ä–æ–ø—É—â–µ–Ω–æ'] > 0]
            if not missing_df.empty: st.dataframe(missing_df, use_container_width=True)
            else: st.success("‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö –Ω–µ–º–∞—î!")
        else:
            st.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É!")
    else:
        if lakes_table is not None and not lakes_table.empty:
            lake_data = None
            for col in ['LakeHouse', 'name', 'Name', '–Ω–∞–∑–≤–∞', '–ù–∞–∑–≤–∞', 'lake_name', 'Lake Name', 'Lakehouse']:
                if col in lakes_table.columns and lake_name in lakes_table[col].values:
                    lake_data = lakes_table[lakes_table[col] == lake_name]
                    break
            if (lake_data is None or lake_data.empty) and 'LakeHouse' in lakes_table.columns:
                uniq = lakes_table['LakeHouse'].dropna().unique()
                if len(uniq) == 1:
                    lake_data = lakes_table
                    st.info(f"üí° –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —î–¥–∏–Ω–∏–π –ª–µ–π–∫: {uniq[0]}")
            if lake_data is not None and not lake_data.empty:
                st.success(f"üèûÔ∏è –í–∏–±—Ä–∞–Ω–æ –ª–µ–π–∫: **{lake_name}**")
                if '–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫' in lake_data.columns and pd.notna(lake_data['–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫'].iloc[0]):
                    st.subheader("‚ÑπÔ∏è –ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫")
                    st.info(lake_data['–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫'].iloc[0])
                if 'Folder' in lake_data.columns:
                    st.subheader("üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ª–µ–π–∫–∞")
                    unique_folders = lake_data['Folder'].dropna().unique()
                    if len(unique_folders) > 0:
                        st.write("**–î–æ—Å—Ç—É–ø–Ω—ñ –ø–∞–ø–∫–∏:**")
                        cols = st.columns(min(3, len(unique_folders)))
                        selected_folder = None
                        for i, folder in enumerate(unique_folders):
                            with cols[i % 3]:
                                if st.button(f"üìÇ {folder}", key=f"folder_{i}"):
                                    selected_folder = folder
                        if selected_folder:
                            st.success(f"üìÇ –í–∏–±—Ä–∞–Ω–æ –ø–∞–ø–∫—É: **{selected_folder}**")
                            folder_data = lake_data[lake_data['Folder'] == selected_folder]
                            st.subheader("üß© –ï–ª–µ–º–µ–Ω—Ç–∏ –ø–∞–ø–∫–∏")
                            display_columns = folder_data.columns[2:8]
                            if 'URL' in display_columns:
                                display_columns = [c for c in display_columns if c != 'URL']
                            if 'Element' in display_columns and 'URL' in folder_data.columns:
                                elements_df_display = folder_data[display_columns].copy()
                                url_dict = {idx: row.get('URL','').strip() for idx, row in folder_data.iterrows()
                                            if pd.notna(row.get('URL','')) and str(row.get('URL','')).strip()}
                                def create_link(row_data):
                                    element_name = row_data['Element']
                                    row_idx = row_data.name
                                    if row_idx in url_dict:
                                        url = url_dict[row_idx]
                                        return f'<a href="{url}" target="_blank" style="color:#1f77b4;text-decoration:underline;">{element_name}</a>'
                                    return element_name
                                elements_df_display['Element'] = elements_df_display.apply(create_link, axis=1)
                                st.markdown(elements_df_display.to_html(escape=False), unsafe_allow_html=True)
                                st.info(f"üîó –ê–∫—Ç–∏–≤–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å: {len(url_dict)}")
                            else:
                                st.dataframe(folder_data[display_columns], use_container_width=True, hide_index=True)
                            st.subheader("üìù –í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω")
                            changes_col = '–í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω'
                            if changes_col in folder_data.columns and pd.notna(folder_data[changes_col].iloc[0]):
                                with st.expander("–ü–æ–∫–∞–∑–∞—Ç–∏ –¥–µ—Ç–∞–ª—ñ –∑–º—ñ–Ω", expanded=True):
                                    process_text_with_images(folder_data[changes_col].iloc[0])
                            else:
                                st.info("–ù–µ–º–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –≤–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω –¥–ª—è —Ü—ñ—î—ó –ø–∞–ø–∫–∏.")
                        else:
                            st.info("üëÜ –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å –Ω–∞ –ø–∞–ø–∫—É –≤–∏—â–µ, —â–æ–± –ø–æ–±–∞—á–∏—Ç–∏ —ó—ó –µ–ª–µ–º–µ–Ω—Ç–∏")
                    else:
                        st.warning("‚ö†Ô∏è –ü–∞–ø–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–∏—Ö")
                else:
                    st.warning("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'Folder' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∫–∞–∑—É—é –≤—Å—ñ –¥–∞–Ω—ñ:")
                    st.dataframe(lake_data, use_container_width=True, hide_index=True)
            else:
                st.error(f"‚ùå –õ–µ–π–∫ '{lake_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        else:
            st.warning("‚ö†Ô∏è –î–∞–Ω—ñ –ª–µ–π–∫—ñ–≤ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ.")

# ==================== –†–ï–î–ê–ì–£–í–ê–ù–ù–Ø –î–ê–ù–ò–• ====================
elif section == "‚úèÔ∏è –†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö":
    st.header("‚úèÔ∏è –†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö")
    if lakes_table is not None and not lakes_table.empty:
        st.subheader("üìä –ü–æ—Ç–æ—á–Ω—ñ –¥–∞–Ω—ñ")
        st.info("üí° –†–µ–¥–∞–≥—É–π—Ç–µ –¥–∞–Ω—ñ –ø—Ä—è–º–æ –≤ —Ç–∞–±–ª–∏—Ü—ñ. –ó–º—ñ–Ω–∏ –±—É–¥—É—Ç—å –∑–∞–ø–∏—Å–∞–Ω—ñ —É Google Sheets; —è–∫—â–æ –Ω–µ –≤–¥–∞—Å—Ç—å—Å—è ‚Äî —É –ª–æ–∫–∞–ª—å–Ω–∏–π Excel (—Ä–µ–∑–µ—Ä–≤).")

        edited_df = st.data_editor(
            lakes_table, use_container_width=True, num_rows="dynamic", key="data_editor"
        )

        if not edited_df.equals(lakes_table):
            # –ø—Ä–æ–±—É–µ–º Google Sheets
            if save_to_google_sheets(edited_df, reports_table):
                st.cache_data.clear()
                time.sleep(1.2)
                st.rerun()
            else:
                # –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∑–µ—Ä–≤
                ok, saved = save_data_to_excel(edited_df, EXCEL_FILE_PATH, reports_table)
                if ok:
                    st.cache_data.clear()
                    time.sleep(1.2)
                    st.rerun()

        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ –û–Ω–æ–≤–∏—Ç–∏ –¥–∞–Ω—ñ"):
                st.cache_data.clear()
                st.rerun()
        with col2:
            csv = (lakes_table if lakes_table is not None else pd.DataFrame()).to_csv(index=False)
            st.download_button("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV", data=csv, file_name=f"lakes_data_{datetime.now().strftime('%Y%m%d')}.csv", mime="text/csv")

        st.subheader("‚ûï –î–æ–¥–∞—Ç–∏ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å")
        with st.form("add_new_record"):
            c1, c2 = st.columns(2)
            with c1:
                new_lakehouse = st.text_input("LakeHouse *")
                new_folder = st.text_input("Folder *")
                new_element = st.text_input("Element *")
                new_url = st.text_input("URL")
            with c2:
                new_info = st.text_area("–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫")
                new_changes = st.text_area("–í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω")
            if st.form_submit_button("‚ûï –î–æ–¥–∞—Ç–∏ –∑–∞–ø–∏—Å"):
                if new_lakehouse and new_folder and new_element:
                    new_row = {
                        'LakeHouse': new_lakehouse,
                        'Folder': new_folder,
                        'Element': new_element,
                        'URL': new_url or '',
                        '–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ª–µ–π–∫': new_info or '',
                        '–í–Ω–µ—Å–µ–Ω–Ω—è –∑–º—ñ–Ω': new_changes or ''
                    }
                    new_df = pd.concat([lakes_table, pd.DataFrame([new_row])], ignore_index=True)

                    if save_to_google_sheets(new_df, reports_table):
                        st.cache_data.clear()
                        time.sleep(1.2)
                        st.rerun()
                    else:
                        st.warning("‚ö†Ô∏è Google Sheets –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ó–±–µ—Ä—ñ–≥–∞—é –ª–æ–∫–∞–ª—å–Ω–æ —è–∫ —Ä–µ–∑–µ—Ä–≤–Ω—É –∫–æ–ø—ñ—é.")
                        ok, saved = save_data_to_excel(new_df, EXCEL_FILE_PATH, reports_table)
                        if ok:
                            st.cache_data.clear()
                            time.sleep(1.2)
                            st.rerun()
                else:
                    st.error("‚ùå –ó–∞–ø–æ–≤–Ω—ñ—Ç—å –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–æ–ª—è: LakeHouse, Folder, Element")
    else:
        st.warning("‚ö†Ô∏è –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —Ä–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è. –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ Excel –∞–±–æ —É–≤—ñ–º–∫–Ω—ñ—Ç—å Google Sheets.")

# ==================== –ö–û–ù–¢–ê–ö–¢–ò –¢–ê –†–ï–°–£–†–°–ò ====================
elif section == "üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–∞ —Ä–µ—Å—É—Ä—Å–∏":
    st.header("üìû –ö–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–∞ —Ä–µ—Å—É—Ä—Å–∏")
    st.subheader("üë• –ù–∞—à–∞ –∫–æ–º–∞–Ω–¥–∞")
    st.markdown("""
    ### üè¢ OurTeam

    **Zhovtiuk Svitlana**  
    –ö–µ—Ä—ñ–≤–Ω–∏–∫ –≥—Ä—É–ø–∏  
    üìß s.zhovtiuk@darnytsia.ua

    **Filatova Oleksandra**  
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∑ –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏  
    üìß oleksandra.filatova@darnytsia.ua

    **Bohdanyk Oleksandr**  
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∑ –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏  
    üìß o.bohdanyk@darnytsia.ua

    **Taranenko Oleksandr**  
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∑ –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏  
    üìß o.taranenko@darnytsia.ua
    """)
    st.subheader("üîó –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("""
        ### –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏:
        - [SharePoint –∫–æ–º–∞–Ω–¥–∏](https://darnytsia.sharepoint.com)
        - [Azure DevOps](https://dev.azure.com/darnitsa)
        - [Power BI Service](https://app.powerbi.com)
        """)
    with c2:
        st.markdown("""
        ### –ó–æ–≤–Ω—ñ—à–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏:
        - [Microsoft Learn](https://learn.microsoft.com)
        - [Power BI Community](https://community.powerbi.com)
        - [Streamlit Docs](https://docs.streamlit.io)
        """)

# ----------------- –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ -----------------


